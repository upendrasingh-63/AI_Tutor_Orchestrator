
# This file defines the state machine and graph for multi-tool execution using LangGraph.

import logging
import asyncio
from typing import List, TypedDict, Annotated, Union
from langgraph.graph import StateGraph, END

from schemas.api_schemas import OrchestratorRequest
from schemas.tool_schemas import ToolCallRequest
from services.semantic_router import SemanticRouter
from services.gemini_executor import GeminiExecutor
from services.tool_executor import ToolExecutor
from utils.exceptions import ToolNotFoundException, ClarificationNeededException

logger = logging.getLogger(__name__)

# --- 1. Define the State for our Graph ---
class GraphState(TypedDict):
    """
    Represents the state of our graph.
    
    Attributes:
        request: The initial orchestrator request.
        candidate_tools: List of tools selected by the semantic router.
        tool_calls: List of tool calls generated by the LLM.
        tool_responses: A list of responses from the executed tools.
        final_response: The final message or content to be sent to the user.
    """
    request: OrchestratorRequest
    candidate_tools: List
    tool_calls: List[ToolCallRequest]
    tool_responses: List[dict]
    final_response: str


class GraphOrchestrator:
    def __init__(self, router: SemanticRouter, gemini_executor: GeminiExecutor, tool_executor: ToolExecutor):
        self.router = router
        self.gemini_executor = gemini_executor
        self.tool_executor = tool_executor
        self.workflow = self._build_graph()

    def _build_graph(self):
        # --- 2. Define the Nodes of the Graph ---
        graph = StateGraph(GraphState)

        graph.add_node("route_tools", self.route_tools_node)
        graph.add_node("generate_tool_calls", self.generate_tool_calls_node)
        graph.add_node("execute_tools", self.execute_tools_node)
        graph.add_node("handle_clarification", self.handle_clarification_node)

        # --- 3. Define the Edges of the Graph ---
        graph.set_entry_point("route_tools")
        graph.add_edge("route_tools", "generate_tool_calls")
        graph.add_conditional_edges(
            "generate_tool_calls",
            self.should_execute_or_clarify,
            {
                "execute": "execute_tools",
                "clarify": "handle_clarification",
                "end_without_action": END,
            }
        )
        graph.add_edge("execute_tools", END)
        graph.add_edge("handle_clarification", END)

        return graph.compile()

    # --- Node Implementations ---
    async def route_tools_node(self, state: GraphState):
        logger.info("--- (Node) Running Semantic Router ---")
        request = state['request']
        candidate_tools = await self.router.select_candidate_tools(request.current_query)
        if not candidate_tools:
            raise ToolNotFoundException("Router did not find any relevant tools.")
        return {"candidate_tools": candidate_tools}

    async def generate_tool_calls_node(self, state: GraphState):
        logger.info("--- (Node) Generating Tool Calls with Gemini ---")
        request = state['request']
        try:
            tool_calls = await self.gemini_executor.get_tool_calls(
                user_query=request.current_query,
                user_info=request.user_info,
                chat_history=request.chat_history,
                candidate_tools=state['candidate_tools']
            )
            return {"tool_calls": tool_calls}
        except ClarificationNeededException as e:
            # Put the clarification request in the state to be handled by the conditional edge
            return {"final_response": e.message}


    async def execute_tools_node(self, state: GraphState):
        logger.info("--- (Node) Executing Tools in Parallel ---")
        tool_calls = state['tool_calls']
        # print(state["tool_calls"])
        # This is where the magic happens: asyncio.gather runs all tool calls concurrently
        tasks = [
            self.tool_executor.execute(call.tool_name, call.parameters)
            for call in tool_calls
        ]
        tool_responses = await asyncio.gather(*tasks, return_exceptions=True)

        # We combine the responses into a single structure for the final output
        return {"tool_responses": list(tool_responses)}

    async def handle_clarification_node(self, state: GraphState):
        logger.info("--- (Node) Handling Clarification ---")
        # The final_response is already set from the generate_tool_calls_node
        return {}

    # --- Conditional Edge Logic ---
    def should_execute_or_clarify(self, state: GraphState) -> str:
        logger.info("--- (Edge) Deciding Next Step ---")
        if state.get("final_response"):
            return "clarify"
        if state.get("tool_calls"):
            return "execute"
        return "end_without_action"
    
    # --- Invocation Method ---
    async def invoke(self, request: OrchestratorRequest) -> dict:
        """
        Runs the graph with the initial request.
        """
        initial_state: GraphState = {
            "request": request,
            "candidate_tools": [],
            "tool_calls": [],
            "tool_responses": [],
            "final_response": ""
        }
        final_state = await self.workflow.ainvoke(initial_state)
        
        # Format the final output based on the graph's terminal state
        if final_state.get("final_response"):
            return {
                "response_type": "clarification",
                "content": None,
                "message": final_state["final_response"]
            }
        
        if final_state.get("tool_responses"):
            return {
                "response_type": "tool_response",
                "content": {"results": final_state["tool_responses"]},
                "message": "Tools executed successfully."
            }
        
        return {
            "response_type": "error",
            "content": None,
            "message": "Orchestration finished without taking any action."
        }
